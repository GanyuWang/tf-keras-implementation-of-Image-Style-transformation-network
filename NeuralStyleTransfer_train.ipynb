{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralStyleTransfer-train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-9aOhXT-5ak"
      },
      "source": [
        "# Image Style Transfer Network\n",
        "\n",
        "Author: Ganyu Wang\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHPDCEJQLCxC"
      },
      "source": [
        "Download intel image classification dataset\n",
        "\n",
        "input your kaggle.json file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqzfbZit3JqR"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload() #upload kaggle.json\n",
        "\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d puneet6060/intel-image-classification\n",
        "!unzip intel-image-classification.zip\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJa6I-eMLNt6"
      },
      "source": [
        "(From Ganyu Wang's Personal Server.)\n",
        "\n",
        "Download the style file\n",
        "\n",
        "Download one content file for qualitative study\n",
        "\n",
        "make folders to save model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K-rmSCpK58z"
      },
      "source": [
        "!wget http://107.172.22.247/NST/impression_sunrise.jpg\n",
        "\n",
        "!wget http://107.172.22.247/NST/3.jpg\n",
        "\n",
        "!mkdir checkpoint\n",
        "!mkdir model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOmYfVEpTHxj"
      },
      "source": [
        "Build the model. Define loss function. Compile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HAzVXPS3E1-"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Nov 27\n",
        "\n",
        "@author: Ganyu Wang\n",
        "\n",
        "Train Draft 4\n",
        "\n",
        "use the dataset intel recognition. \n",
        "\n",
        "train a identify transformation model. \n",
        "\n",
        "save model, reconstructruct model.\n",
        "\n",
        "Write the custom loss function again. \n",
        "    the custom loss function should return the \n",
        "\n",
        "Predict.\n",
        "Visualization. \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import PIL.Image\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#assert tf.test.is_gpu_available()\n",
        "#assert tf.test.is_built_with_cuda()\n",
        "\n",
        "tf.test.gpu_device_name()\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        'seg_train/seg_train',\n",
        "        target_size=(256, 256),\n",
        "        batch_size=32,\n",
        "        class_mode= \"input\",\n",
        "        shuffle=False)\n",
        "\n",
        "\n",
        "#%% build models\n",
        "\n",
        "def residual_block(y, nb_channels, _strides=(1, 1), _project_shortcut=False):\n",
        "    shortcut = y\n",
        "\n",
        "    # down-sampling is performed with a stride of 2\n",
        "    y = layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
        "    y = layers.BatchNormalization()(y)\n",
        "    y = layers.LeakyReLU()(y)\n",
        "\n",
        "    y = layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=(1, 1), padding='same')(y)\n",
        "    y = layers.BatchNormalization()(y)\n",
        "\n",
        "    # identity shortcuts used directly when the input and output are of the same dimensions\n",
        "    if _project_shortcut or _strides != (1, 1):\n",
        "        # when the dimensions increase projection shortcut is used to match dimensions (done by 1Ã—1 convolutions)\n",
        "        # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
        "        shortcut = layers.Conv2D(nb_channels, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
        "        shortcut = layers.BatchNormalization()(shortcut)\n",
        "\n",
        "    y = layers.add([shortcut, y])\n",
        "    y = layers.LeakyReLU()(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "def Identical_transform_model(y):\n",
        "    \"\"\"\n",
        "    transform_model, channel last, \n",
        "        input  (batch_size, width, height, channel) -> (BS, 256, 256, 3)\n",
        "        output (batch_size, width, height, channel) -> (BS, 256, 256, 3)\n",
        "    \n",
        "    \"\"\"\n",
        "    d = Conv2D(32, (9, 9), input_shape=(256, 256, 3), padding='same', activation='relu')(y)\n",
        "    d = Conv2D(64, (3, 3), activation='relu', padding='same', strides=2)(d)\n",
        "    d = Conv2D(128, (3, 3), activation='relu', padding='same', strides=2)(d)\n",
        "    d = residual_block(d, 128)\n",
        "    d = residual_block(d, 128)\n",
        "    d = residual_block(d, 128)\n",
        "    d = residual_block(d, 128)\n",
        "    d = residual_block(d, 128)\n",
        "    d = Conv2DTranspose(64, (3, 3), activation='relu', padding='same', strides=2)(d)\n",
        "    d = Conv2DTranspose(32, (3, 3), activation='relu', padding='same', strides=2)(d)\n",
        "    d = Conv2DTranspose(3, (9, 9), activation='relu', padding='same', strides=1)(d)\n",
        "    \n",
        "    return d\n",
        "\n",
        "inputs = keras.Input(shape=(256, 256, 3))\n",
        "outputs = Identical_transform_model(inputs)\n",
        "transform_model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "\n",
        "#%% define loss function. \n",
        "\n",
        "# VGG layers.\n",
        "def vgg_layers(layer_names):\n",
        "    \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n",
        "    # Load our model. Load pretrained VGG, trained on imagenet data\n",
        "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "    vgg.trainable = False\n",
        "    outputs = [vgg.get_layer(name).output for name in layer_names]\n",
        "    model = tf.keras.Model([vgg.input], outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_image(path, return_type=\"numpy\"):\n",
        "    \"\"\"\n",
        "    load image from path\n",
        "        resize to (1, 256, 256, 3)\n",
        "        \n",
        "    \"\"\"\n",
        "    t = PIL.Image.open(path)\n",
        "    t = t.resize((256, 256))\n",
        "    t = np.asarray(t)\n",
        "    t = t/256\n",
        "    t = t.reshape(1, 256, 256, 3)\n",
        "    \n",
        "    if return_type == \"tensor\":\n",
        "        return tf.constant(t, dtype=\"float32\")\n",
        "    return t\n",
        "\n",
        "\n",
        "# get content loss and style loss from VGG net. \n",
        "content_layers = ['block5_conv2'] \n",
        "num_content_layers = len(content_layers)\n",
        "# style loss.  \n",
        "style_image = load_image(\"impression_sunrise.jpg\", return_type='tensor')\n",
        "style_layers = ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1', \n",
        "                'block4_conv1', \n",
        "                'block5_conv1']\n",
        "num_style_layers = len(style_layers)\n",
        "\n",
        "# extra\n",
        "style_extractor = vgg_layers(style_layers)\n",
        "style_outputs = style_extractor(style_image*255)\n",
        "\n",
        "#\n",
        "def gram_matrix(input_tensor):\n",
        "    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
        "    input_shape = tf.shape(input_tensor)\n",
        "    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
        "    return result/(num_locations)\n",
        "\n",
        "\n",
        "class StyleContentModel(tf.keras.models.Model):\n",
        "    def __init__(self, style_layers, content_layers):\n",
        "        super(StyleContentModel, self).__init__()\n",
        "        self.vgg =  vgg_layers(style_layers + content_layers)\n",
        "        self.style_layers = style_layers\n",
        "        self.content_layers = content_layers\n",
        "        self.num_style_layers = len(style_layers)\n",
        "        self.vgg.trainable = False\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        \"Expects float input in [0,1]\"\n",
        "        inputs = inputs*255.0\n",
        "        #preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs) # problem here\n",
        "        preprocessed_input = inputs\n",
        "        # eliminated preprocess\n",
        "        \n",
        "        outputs = self.vgg(preprocessed_input)\n",
        "        style_outputs, content_outputs = (outputs[:self.num_style_layers], \n",
        "                                          outputs[self.num_style_layers:])\n",
        "      \n",
        "        style_outputs = [gram_matrix(style_output)\n",
        "                         for style_output in style_outputs]\n",
        "      \n",
        "        content_dict = {content_name:value \n",
        "                        for content_name, value \n",
        "                        in zip(self.content_layers, content_outputs)}\n",
        "      \n",
        "        style_dict = {style_name:value\n",
        "                      for style_name, value\n",
        "                      in zip(self.style_layers, style_outputs)}\n",
        "        \n",
        "        return {'content':content_dict, 'style':style_dict}\n",
        "\n",
        "extractor = StyleContentModel(style_layers, content_layers)\n",
        "\n",
        "\n",
        "style_weight=1e-2\n",
        "content_weight=1e5\n",
        "\n",
        "\n",
        "# content input batch, the data from data generator, [batch, 256, 256, 3]\n",
        "def content_loss(content_input, transform_model_output):\n",
        "    content_input_target = extractor(content_input)['content']  # a dict, content\n",
        "    content_trans_target = extractor(transform_model_output)['content']\n",
        "    content_loss = tf.add_n([tf.reduce_mean((content_trans_target[name]-content_input_target[name])**2, [1,2,3]) \n",
        "                             for name in content_trans_target.keys()])\n",
        "    content_loss /= num_content_layers     # divided by number of layers to standardlize the weight.\n",
        "    return content_loss\n",
        "\n",
        "# style input is the image tf tensor (1, width, height, 3).\n",
        "def style_loss(style_input, transform_model_output):\n",
        "    style_input_target = extractor(style_input)['style'] # a dict, {style_layer: tensor }\n",
        "    style_trans_target = extractor(transform_model_output)['style']\n",
        "    style_loss = tf.add_n([tf.reduce_mean((style_trans_target[name]-style_input_target[name])**2, [1,2]) \n",
        "                           for name in style_trans_target.keys()])\n",
        "    style_loss /= num_style_layers\n",
        "    return style_loss\n",
        "    \n",
        "\n",
        "def style_content_loss(content_input, transform_model_output):\n",
        "\n",
        "    total_loss = content_weight * content_loss(content_input, transform_model_output) \\\n",
        "            + style_weight * style_loss(style_image, transform_model_output) \n",
        "    return total_loss\n",
        "\n",
        "\n",
        "#%% compile. \n",
        "transform_model.compile(\n",
        "                optimizer='adam',\n",
        "                loss=style_content_loss\n",
        "              )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZZVEM9DnbQ6"
      },
      "source": [
        "#%% train. \n",
        "# this procedure can be run several times. \n",
        "\n",
        "\n",
        "checkpoint_filepath = 'checkpoint/'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True\n",
        "    )\n",
        "\n",
        "\n",
        "transform_model.fit(train_generator, epochs=40, callbacks=[model_checkpoint_callback])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa7mF8RdayTx"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "#%% save model\n",
        "transform_model.save(\"model/transform_model\")\n",
        "transform_model.save(\"model/transform_model.h5\")\n",
        "\n",
        "#files.download('model/transform_model') \n",
        "files.download('model/transform_model.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8gKJ37KYZs6"
      },
      "source": [
        "Use original model to transfer an image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1AQc6WfVKkH"
      },
      "source": [
        "test_img = load_image(\"3.jpg\")\n",
        "pred_img = transform_model.predict(test_img)\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(test_img[0,])\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(pred_img[0,])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eeygt9HpZtWq"
      },
      "source": [
        "Use reconstruct model to predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeRlfHNCNrbw"
      },
      "source": [
        "#%% save and reconstruct the model. \n",
        "reconstruct_model = keras.models.load_model(\"model/transform_model\", compile=False)\n",
        "\n",
        "#%% predict \n",
        "\n",
        "test_img = load_image(\"3.jpg\")\n",
        "pred_img = transform_model.predict(test_img)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(test_img[0,])\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(pred_img[0,])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}